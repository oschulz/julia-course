{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Introduction to Bayesian Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial introduces the reader to the concept of Bayesian neural networks. A symbiosis between the extensive research done on neural networks, but where at the same time hyperparameters and \"prior\" knowledge are encoded in the setup of the weights, and Bayesian analysis, the study of the relation between our preconceptions and their change when confronted with new data. In its simplest form one would take distributions from our favorite package and put them over the weights of the neural network.\n",
    "\n",
    "<img src=\"presentation/imgs/BayesianNN.png\" width=\"500\" height=\"170\" />\n",
    "\n",
    "As the search for a posterior does then, like in the probabilistic programming tutorial, rely on sampling-heavy routines the embedding of such Bayesian neural network inside of a PPL, such as Turing allows Turing's advanced inference algorithms to sample from the probabilistic model and obtain a posterior. We are at the same time faced with the same dilemma as in the amortized inference example; we need to evaluate our desire for accuracy against the available compute ressources and the desire to reduce the influence of priors' influence on the training outcome.\n",
    "\n",
    "### Main advantages of Bayesian Neural Networks:\n",
    "\n",
    "- Ability to include and quantify uncertainties\n",
    "- Improve robustness against adversarial examples\n",
    "\n",
    "### Downside of Bayesian Neural Networks:\n",
    "\n",
    "- Computational cost\n",
    "\n",
    "For a brief walk through the mathematical formalism, please have a look at [this](https://davidstutz.de/a-short-introduction-to-bayesian-neural-networks/) blog entry. For a much deeper look into the topic feel free to have a look Radford M. Neal's PhD thesis [Bayesian Learning for Neural Networks](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.446.9306&rep=rep1&type=pdf).\n",
    "\n",
    "\n",
    "## Outline:\n",
    "\n",
    "**Section 1.** [A First Bayesian Neural Network](#first)\n",
    "\n",
    "**Section 2.** [Generic Bayesian Neural Networks](#generic)\n",
    "\n",
    "**Section 3.** [Exercise - Different types of Bayesian Neural Networks](#ex)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Turing, Flux, Plots, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide sampling progress\n",
    "Turing.turnprogress(false);\n",
    "\n",
    "# Use reverse_diff due to the number of parameters in the neural network\n",
    "Turing.setadbackend(:reverse_diff);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: A First Bayesian Neural Network <a name=\"first\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an artificial dataset with its points arranged in a box-like pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of points\n",
    "N = 80\n",
    "M = round(Int, N / 4)\n",
    "Random.seed!(1234)\n",
    "\n",
    "# Generate artificial data\n",
    "x1s = rand(M) * 4.5; x2s = rand(M) * 4.5;\n",
    "xt1s = Array([[x1s[i] + 0.5; x2s[i] + 0.5] for i = 1:M])\n",
    "x1s = rand(M) * 4.5; x2s = rand(M) * 4.5;\n",
    "append!(xt1s, Array([[x1s[i] - 5; x2s[i] - 5] for i = 1:M]))\n",
    "\n",
    "x1s = rand(M) * 4.5; x2s = rand(M) * 4.5;\n",
    "xt0s = Array([[x1s[i] + 0.5; x2s[i] - 5] for i = 1:M])\n",
    "x1s = rand(M) * 4.5; x2s = rand(M) * 4.5;\n",
    "append!(xt0s, Array([[x1s[i] - 5; x2s[i] + 0.5] for i = 1:M]))\n",
    "\n",
    "# Store all data for later use\n",
    "xs = [xt1s; xt0s]\n",
    "ts = [ones(2*M); zeros(2*M)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the artificial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data points\n",
    "function plot_data()\n",
    "    x1 = map(e -> e[1], xt1s)\n",
    "    y1 = map(e -> e[2], xt1s)\n",
    "    x2 = map(e -> e[1], xt0s)\n",
    "    y2 = map(e -> e[2], xt0s)\n",
    "    \n",
    "    Plots.scatter(x1, y1, color=\"red\", clim = (0, 1))\n",
    "    Plots.scatter!(x2, y2, color=\"blue\", clim = (0, 1))\n",
    "end\n",
    "\n",
    "plot_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Create a neural network with two hidden layers and one output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function for the training of neural networks and subsequently construct the neural network in Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a vector into a set of weights and biases\n",
    "function unpack(nn_params::AbstractVector)\n",
    "    W1 = reshape(nn_params[1:6], 3, 2);\n",
    "    b1 = reshape(nn_params[7:9], 3)\n",
    "    \n",
    "    W2 = reshape(nn_params[10:15], 2, 3);\n",
    "    b2 = reshape(nn_params[16:17], 2)\n",
    "    \n",
    "    W0 = reshape(nn_params[18:19], 1, 2);\n",
    "    b0 = reshape(nn_params[20:20], 1)\n",
    "    return W1, b1, W2, b2, W0, b0\n",
    "end\n",
    "\n",
    "# Construct the neural network with Flux and predict its output\n",
    "function nn_forward(xs, nn_params::AbstractVector)\n",
    "    W1, b1, W2, b2, W0, b0 = unpack(nn_params)\n",
    "    nn = Chain(Dense(W1, b1, tanh),\n",
    "               Dense(W2, b2, tanh),\n",
    "               Dense(W0, b0, σ))\n",
    "    return nn(xs)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Build the probabilistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the probabilistic model encapsulating the Bayesian neural network, where the prior comes from a multivariate normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a regularization term and a Gaussian prior variance term\n",
    "alpha = 0.09\n",
    "sig = sqrt(1.0 / alpha)\n",
    "\n",
    "# Specify the probabilistic model\n",
    "@model bayes_nn(xs, ts) = begin\n",
    "    # Create the weight and bias vector\n",
    "    nn_params ~ MvNormal(zeros(20), sig .* ones(20))\n",
    "    \n",
    "    # Calculate predictions for the inputs given the weights and biases in theta\n",
    "    preds = nn_forward(xs, nn_params)\n",
    "    \n",
    "    # Observe each prediction\n",
    "    for i = 1:length(ts)\n",
    "        ts[i] ~ Bernoulli(preds[i])\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform inference using the Hamiltonian Monte-Carlo algorithm with 5000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on the Bayesian neural network\n",
    "N = 5000\n",
    "ch = sample(bayes_nn(hcat(xs...), ts), HMC(0.05, 4), N);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the posterior values for the weights and biases from the sampled chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights and biases\n",
    "theta = ch[:nn_params].value.data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: Maximum a posteriori (MAP) estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the set of weights, which provided the highest log posterior and subsequently plot the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data()\n",
    "\n",
    "# Find index with highest log posterior in the chain\n",
    "_, i = findmax(ch[:lp].value.data)\n",
    "\n",
    "# Extract max row value\n",
    "i = i.I[1]\n",
    "\n",
    "# Plot posterior distribution\n",
    "x_range = collect(range(-6, stop=6, length=25))\n",
    "y_range = collect(range(-6, stop=6, length=25))\n",
    "Z = [nn_forward([x, y], theta[i, :])[1] for x=x_range, y=y_range]\n",
    "contour!(x_range, y_range, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawn from the MCMC chain the predicted values are returned after the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return average predicted value across weights\n",
    "function nn_predict(x, theta, num)\n",
    "    mean([nn_forward(x, theta[i, :])[1] for i in 1:10:num])\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average prediction\n",
    "plot_data()\n",
    "\n",
    "n_end = 1500\n",
    "x_range = collect(range(-6, stop=6, length=25))\n",
    "y_range = collect(range(-6, stop=6, length=25))\n",
    "Z = [nn_predict([x, y], theta, n_end)[1] for x=x_range, y=y_range]\n",
    "contour!(x_range, y_range, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ability to animate the sampling process we visualize the change in posterior over time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the evolution of the network's predictive power\n",
    "n_end = 500\n",
    "\n",
    "anim = @animate for i=1:n_end\n",
    "    plot_data()\n",
    "    Z = [nn_forward([x, y], theta[i, :])[1] for x=x_range, y=y_range]\n",
    "    contour!(x_range, y_range, Z, title=\"Iteration $i\", clim=(0, 1))\n",
    "end every 5\n",
    "\n",
    "gif(anim, \"/tmp/jl_ozeq2f.gif\", fps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Generic Bayesian Neural Networks <a name=\"generic\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a more general setup of the neural network, which will allow us to change subparts of the network later on in the exercises. The main constraint here is that we are still constrained to purely 'Dense' layers and have to refrain from using more advanced cells in this framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 More generalized framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We govern the shape of the entire network through the 'network_shape'. Data is subsequently prepared for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the network architecture made up of 'dense' layers\n",
    "network_shape = [\n",
    "    (3, 2, :tanh),\n",
    "    (2, 3, :tanh),\n",
    "    (1, 2, :σ)\n",
    "]\n",
    "\n",
    "# Regularization, parameter variance & total number of parameters\n",
    "alpha = 0.09\n",
    "sig = sqrt(1.0 / alpha)\n",
    "num_params = sum([i * o + i for (i, o, _) in network_shape])\n",
    "\n",
    "# Generate a series of vectors given the network shape\n",
    "function unpack(θ::AbstractVector, network_shape::AbstractVector)\n",
    "    index = 1\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for layer in network_shape\n",
    "        rows, cols, _ = layer\n",
    "        size = rows * cols\n",
    "        last_index_w = size + index - 1\n",
    "        last_index_b = last_index_w + rows\n",
    "        push!(weights, reshape(θ[index:last_index_w], rows, cols))\n",
    "        push!(biases, reshape(θ[last_index_w + 1:last_index_b], rows))\n",
    "        index = last_index_b + 1\n",
    "    end\n",
    "    return weights, biases\n",
    "end\n",
    "\n",
    "# Generate the neural network given a shape and return a prediction\n",
    "function nn_forward(x, θ::AbstractVector, network_shape::AbstractVector)\n",
    "    weights, biases = unpack(θ, network_shape)\n",
    "    layers = []\n",
    "    for i in eachindex(network_shape)\n",
    "        push!(layers, Dense(weights[i],\n",
    "                biases[i],\n",
    "                eval(network_shape[i][3])))\n",
    "    end\n",
    "    nn = Chain(layers...)\n",
    "    return nn(x)\n",
    "end\n",
    "\n",
    "# General Turing specification for a BNN\n",
    "@model bayes_nn(xs, ts, network_shape, num_params) = begin\n",
    "    θ ~ MvNormal(zeros(num_params), sig .* ones(num_params))\n",
    "    preds = nn_forward(xs, θ, network_shape)\n",
    "    for i = 1:length(ts)\n",
    "        ts[i] ~ Bernoulli(preds[i])\n",
    "    end\n",
    "end\n",
    "\n",
    "# Set the backend\n",
    "Turing.setadbackend(:reverse_diff);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Hamiltonian Monte-Carlo to sample from the probabilistic neural network model and arrive at a valid posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference\n",
    "num_samples = 10000\n",
    "ch2 = sample(bayes_nn(hcat(xs...), ts, network_shape, num_params), HMC(0.05, 4), num_samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions based on the network shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions based on network shape\n",
    "function nn_predict(x, theta, num, network_shape)\n",
    "    mean([nn_forward(x, theta[i, :], network_shape)[1] for i in 1:10:num])\n",
    "end;\n",
    "\n",
    "# Extract θ from the sampled chain\n",
    "params2 = ch2[:θ].value.data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the prediction\n",
    "plot_data()\n",
    "\n",
    "x_range = collect(range(-6, stop=6, length=25))\n",
    "y_range = collect(range(-6, stop=6, length=25))\n",
    "Z = [nn_predict([x, y], params2, length(ch2), network_shape)[1] for x=x_range, y=y_range]\n",
    "contour!(x_range, y_range, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Exercise - Different types of Bayesian Neural Networks <a name=\"ex\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Experiment with deep Bayesian neural networks\n",
    "    - How does the inference engine scale throughout changes to the neural networ?\n",
    "- Experiment with different kinds of noise by rewriting the probabilistic model using the above examples and Turing's [library](https://github.com/TuringLang/Turing.jl/blob/master/src/stdlib/distributions.jl).\n",
    "- How many samples do I need across networks to arrive at an expressive posterior?\n",
    "- Change the inference algorithm employed by Turing to a variational inference based one\n",
    "- Transfer the linear regression and logistic regression frameworks from the previous tutorials into the Bayesian machine learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
